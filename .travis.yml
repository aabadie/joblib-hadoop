sudo: required

language: python

python:
    - "3.5"

services:
    - docker

env:
  DOCKER_COMPOSE_VERSION: 1.11.2

before_install:
    # Install latest available version of docker
    - sudo apt-get update
    - sudo apt-get install -o Dpkg::Options::="--force-confold" --force-yes -y docker-engine make

    # Install a more recent version of docker-compose
    - sudo rm /usr/local/bin/docker-compose
    - curl -L https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-`uname -s`-`uname -m` > docker-compose
    - chmod +x docker-compose
    - sudo mv docker-compose /usr/local/bin

    # Clean up docker cache and start the Hadoop cluster
    - cd docker
    - docker system prune -af
    - docker-compose up -d
    - cd ..

install:
    # Install miniconda environment
    - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
    - bash miniconda.sh -b -p $HOME/miniconda
    - export PATH="$HOME/miniconda/bin:$PATH"
    - conda config --set always_yes yes --set changeps1 no
    - conda update -y -q conda
    - conda create -y -q -n test-environment python=$TRAVIS_PYTHON_VERSION libhdfs3 -c conda-forge
    - source activate test-environment

    # Install joblib-hadoop
    - pip install -r requirements.txt .

    # Some checks on docker and Hadoop cluster health
    - docker-compose --version
    - docker --version
    - docker ps
    - docker network ls
    - docker network inspect docker_default
    - cd docker && docker-compose ps && cd ..

script:
    # Run local unit tests first
    - make test

    # Run HDFS store backend, YARN parallel backend examples and pytest
    # are run from the joblib-hadoop-client docker container
    - make run-all

after_success:
    # Send coverage results to codecov
    - codecov

after_failure:
    - make docker-stop

after_script:
    - make docker-stop

notifications:
      email: false
